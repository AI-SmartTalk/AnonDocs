# Server Configuration
PORT=3000
NODE_ENV=development

# LLM Provider Configuration
# Choose your default provider: openai, anthropic, or ollama
DEFAULT_LLM_PROVIDER=ollama

# Ollama Configuration (local LLMs)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=mistral-nemo
OLLAMA_TEMPERATURE=0

# OpenAI Configuration (optional)
# OPENAI_API_KEY=your-api-key-here
# OPENAI_MODEL=gpt-4
# OPENAI_TEMPERATURE=0
# OPENAI_BASE_URL=https://api.openai.com/v1  # Optional: for OpenAI-compatible APIs

# Chunking Configuration
CHUNK_SIZE=1500
CHUNK_OVERLAP=0
ENABLE_PARALLEL_CHUNKS=false
